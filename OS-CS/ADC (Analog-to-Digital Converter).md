디지털(0과 1)의 세계에서 아날로그(연속된 자연)의 세계로 나아갈 준비

# ADC: 자연을 숫자로 번역하는 기계
세상의 온도는 `25도`에서 갑자기 `26도`로 점프하지 않음. `25.1234...` 처럼 연속적으로 변함. 하지만 컴퓨터는 0과 1밖에 모름.

ADC는 이 연속적인 전압의 파도(Wave)를 딱딱 끊어진 **계단(Digital Value)** 으로 만드는 장치.

이 과정에서 가장 중요한 두 가지 스펙 -> 데이터시트에서 ADC를 고를 때 이것만 보면 됨.

# 핵심 스펙 1: 분해능 (Resolution) - "얼마나 섬세한가?"
**"전압을 몇 개의 계단으로 쪼갤 것인가?"** 를 의미함. 임베디드에서는 주로 12-bit ADC를 사용함.
- 비트 수(Bit Depth): 12비트 = $2^{12} = 4096$ 단계
- 기준 전압 (Vref): 보통 3.3V
- 최소 눈금(LSB): $3.3V \div 4096 \approx 0.0008V(0.8mV)$
의미:
- 전압이 `0.8mV` 변할 때마다 ADC 값은 `1`씩 증가함.
- 만약 `2.4mV` 만큼 미세하게 변하는 센서라면? ADC 값은 `3`만큼 변하므로 감지할 수 있음.
- 하지만 `0.1mV`단위로 변하는 초정밀 센서라면? 12비트 ADC로는 변화를 감지할 수 없음. (더 높은 비트 수의 ADC가 필요)

# 핵심 스펙 2: 샘플링 속도 (Sampling Rate) - "얼마나 자주 보는가?"
"1초에 몇 번 측정할 것인가?" (SPS: Samples Per Second)
- 온도 센서: 온도는 천천히 변하므로 1초에 10번(10 SPS)만 봐도 충분함.
- 마이크(오디오): 목소리는 매우 빠르게 진동함. 최소 1초에 44,100번(44.1kHz)은 봐야 소리를 복원할 수 있음.
- 모터 전류 제어: 모터가 타기 전에 과전류를 잡으려면 매우 빨라야 함. (수십~수백 kHz)

	나이퀴스트 이론 (간단 버전): 내가 측정하려는 신호 주파수의 **최소 2배 이상** 빠르게 찍어야 원래 신호를 잃어버리지 않음

# 하드웨어적 난관: 샘플 앤 홀드 (Sample & Hold)와 임피던스
이 부분이 소프트웨어 개발자들이 가장 많이 실수하는 하드웨어 특성.

ADC 내부에는 전압을 잠깐 담아두는 **작은 물컵(Capacitor)** 이 있음.
1. 스위치 ON (Sample): 핀과 내부 커패시터를 연결함. 외부 전압이 커패시터에 충전됨.
2. 스위치 OFF (Hold): 연결을 끊고, 커패시터에 담긴 전압을 잼.
문제 상황: 외부 센서의 힘(출력 임피던스)이 약하면, **커패티서에 물(전압)이 다 차기도 전에 스위치를 닫아버리는 일** 이 발생함.
- 증상: 분명 3V가 들어오는데, ADC 값은 2.5V처럼 낮게 나옴.
- 해결:
	1. 샘플링 시간(Sampling Time) 늘리기: 레지스터 설정으로 "물 채울 시간"을 더줌. (속도는 느려짐)
	2. 하드웨어 버퍼(Op-Amp) 추가: 센서와 ADC 사이에 힘센 앰프를 달아줌.

# 실전 최적화: ADC + DMA의 조합
ADC도 변환이 끝나면 인터럽트를 검. 만약 1MHz(1초에 100만 번)로 샘플링 한다면? CPU는 인터럽트 받다가 아무것도 못 하고 죽음.

그래서 고수들은 이렇게 짬.
1. ADC 설정: "타이머 트리거에 맞춰서 자동으로 1000번 측정해."
2. DMA 설정: "ADC가 변활할 때마다 값을 메모리 배열 `adc_buffer[]"에 차곡차곡 쌓아놔.
3. CPU: "1000개 다 차면 나한테 알려줘." (그동안 다른 일 함)

# 요약
1. ADC는 아날로그 전압을 디지털 숫자로 바꿈.
2. **분해능(Bit)** 은 정밀도를, **샘플링 속도(Hz)** 는 반응 속도를 결정함.
3. 샘플링 시간을 충분히 주지 않으면 값이 왜곡됨. (하드웨어 특성)
4. 빠른 ADC는 반드시 DMA와 함께 써야 함.

