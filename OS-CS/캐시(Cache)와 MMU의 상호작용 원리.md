CPU 내부의 캐시(L1, L2)는 매우 빠르기 때문에, 데이터를 가져올 때마다 느린 MMU 변환 과정을 거치면 속도가 떨어짐. 따라서 효율성을 위해 두 장치는 동시에 작동함.

# TLB (Translation Lookaside Buffer): 주소 변환 캐시
MMU가 매번 페이지 테이블(Page Table)이라는 큰 지도를 참고하면 느려짐. 그래서 MMU는 최근에 변환했던 주소를 TLB라는 작은 캐시에 저장해 둠.
- TLB의 역할: 가상 주소 <-> 물리 주소의 매핑 정보를 임시 저장하여 주소 변환 속도를 극도로 높임.

# 캐시의 주소 결정 방식 (VIPT vs PIPT)
캐시가 데이터를 저장할 때 '가상 주소'를 쓸지, '물리 주소'를 쓸지에 따라 두 가지 주요 방식이 있음.

#### VIPT (Virtually Indexed, Physically Tagged) - 가장 흔함
대부분의 현대 CPU (특히 ARM 기반 임베디드 코어)가 이 방식을 사용함.
1. Index (색인): 캐시의 어느 칸에 데이터가 있을지 찾을 때는 가상 주소를 사용함. (매우 빠름, MMU 기다릴 필요 없음)
2. Tag (태그): 데이터가 맞는지 최종 확인(Hit/Miss 판별)할 때는 물리 주소를 사용함.

- 동작 흐름: CPU는 캐시에 데이터를 요청할 때 MMU와 캐시를 거의 동시에 돌림. 캐시가 빠르게 주소를 찾고(Index), MMU는 동시에 주소를 변환함. 변환된 물리 주소(Tag)를 캐시 데이터와 비교하여 일치하면 데이터를 내어줌.

#### PIPT (Physically Indexed, Physically Tagged)
- 원리: 캐시 접근 전에 무조건 MMU를 거쳐 물리 주소로 변환한 후 캐시를 찾음.
- 단점: 매번 변환 과정을 거치므로 느림. (현대 CPU에서는 거의 쓰이지 않음)

# 결론: "연속된 가상 주소"의 이점
캐시는 실제로 **가상 메모리의 연속된 주소** 를 바탕으로 데이터를 가져오는 것이 맞음.
- CPU 관점: "프로그램은 연속된 `0x1000, 0x1004, 0x1008...` 주소를 요청한다."
- 캐시 관점: "요청이 연속되니, 다음 주소도 연속해서 가져와서 미리 저장해 둬야지." (프리페치, Prefetch)
- MMU 관점: "이 가상 주소들이 실제 물리 메모리에서는 뿔뿔이 흩어져 있어도 상관없어. 내가 정확히 매핑해 줄게."

이 복잡한 과정을 통해 시스템은 **안전성(MMU)** 과 **속도(Cache/TLB)** 라는 두 마리 토끼를 모두 잡음.

