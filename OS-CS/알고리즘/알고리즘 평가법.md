좋은 알고리즘인지 평가하는 법

#### 알고리즘 효율성의 척도: 복잡도 (Complexity)
사용자 1명일 때는 잘 돌아가다가, 100만 명이 되자마자 서버가 멈춘다면 그것은 실패한 설계. 이를 미리 예측하기 위해 입력 크기($n$)가 커질 때, 소요 시간이나 메모리가 얼마나 늘어나는지를 수학적으로 표현.
- 시간 복잡도 (Time Complexity): 연산 횟수가 몇 번 늘어나는가? (속도)
- 공간 복잡도 (Space Complexity): 메모리를 얼마나 더 차지하는가? (용량)

#### 점근  표기법 (Asymptotic Notation)
우리는 정확한 실행 시간(예: 3.52초)을 측정하려는 것이 아님. 컴퓨터 성능에 따라 시간은 달라지기 때문. 우리는 **증가 추세(Trend)** 를 봄. 이를 위해 3가지 표기법을 사용함.
1. Big-O ($O$): 최악의 경우 (Worst Case). "아무리 운이 나빠도 이 정도 시간 안에는 끝난다." -> 엔지니어에게 가장 중요한 지표 (상한선).
2. Big-Omega ($\Omega$): 최선의 경우 (Best Case). "운이 정말 좋으면 이만큼 빨리 끝날 수도 있다." {하한선}.
3. Big-Theta ($\Theta$): 평균적인 경우 (Average Case).

#### 주요 복잡도 클래스 (직관적 이해)
| 표기 ($O$)      | 이름    | 직관적 의미                                        | 대표적인 예시                              |
| ------------- | ----- | --------------------------------------------- | ------------------------------------ |
| $O(1)$        | 상수 시간 | 데이터가 얼마나 많든 즉시 끝난다.                           | 해시 테이블 조회, 스택 Push/Pop               |
| $O(\log n)$   | 로그 시간 | 데이터가 2배 늘어나면, 단계는 고작 +1 늘어난다. (매우 효율적)        | 이진 탐색, B+ Tree 탐색, Skip List         |
| $O(n)$        | 선형 시간 | 데이터가 10배 늘어나면, 시간도 10배 늘어난다.                  | for문 1번, 연결 리스트 순차 탐색                |
| $O(n \log n)$ | 선형 로그 | 데이터를 하나씩 보긴 보는데($n$), 효율적으로 쪼개서 본다($\log n$). | Quick Sort, Merge Sort (일반적인 효율적 정렬) |
| $O(n^2)$      | 이차 시간 | 데이터가 10배 늘어나면, 시간은 100배 늘어난다. (비효율적)          | 이중 for문, 버블 정렬                       |
| $O(2^n)$      | 지수 시간 | 데이터가 조금만 늘어나도 우주 멸망 때까지 계산 안 끝남.              | 암호 해독(Brute Force), 피보나치 재귀(최적화 전)   |

#### 왜 이것이 중요한가?
예를 들어 Consistent Hashing을 배울 때, 서버 추가 시 데이터 이동이 $O(1/N)$이라고 함. 만약 이것이 $O(N)$이었다면, 서버가 1000대일 때 한 대만 추가해도 전체 데이터가 요동쳤을 것. 즉, 복잡도 분석은 시스템이 확장될 때 (Scale-out) 생존할 수 있는지를 판가름하는 생존 함수임.
